{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mg6j7yZJ35L",
        "outputId": "592a13de-7a4d-43c8-c698-cf2ddf218fb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUESTION 1 DATA PREPROCESSING"
      ],
      "metadata": {
        "id": "IHPWkEk0MWA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import string\n",
        "import nltk\n",
        "import random\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Ensure NLTK resources are downloaded (do this once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Path to the dataset folder on Google Drive\n",
        "dataset_path = '/content/drive/My Drive/IR ASSIGNMENT/text_files'\n",
        "preprocessed_path = '/content/drive/My Drive/IR ASSIGNMENT/preprocessed_text_files'\n",
        "\n",
        "# Create a directory for preprocessed files if it doesn't exist\n",
        "if not os.path.exists(preprocessed_path):\n",
        "    os.makedirs(preprocessed_path)\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
        "    # Remove punctuation\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Preprocess all files\n",
        "all_files = os.listdir(dataset_path)\n",
        "preprocessed_files = []\n",
        "\n",
        "for filename in all_files:\n",
        "    file_path = os.path.join(dataset_path, filename)\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        original_text = file.read()\n",
        "\n",
        "        preprocessed_text = preprocess_text(original_text)\n",
        "\n",
        "        # Save the preprocessed text\n",
        "        preprocessed_file_path = os.path.join(preprocessed_path, filename)\n",
        "        with open(preprocessed_file_path, 'w', encoding='utf-8') as preprocessed_file:\n",
        "            preprocessed_file.write(preprocessed_text)\n",
        "            preprocessed_files.append(preprocessed_file_path)\n",
        "\n",
        "print(\"Preprocessing complete for all files.\")\n",
        "\n",
        "# Randomly select 5 preprocessed files and print their contents\n",
        "selected_files = random.sample(preprocessed_files, 5)\n",
        "\n",
        "for file_path in selected_files:\n",
        "    filename = os.path.basename(file_path)\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        preprocessed_text = file.read()\n",
        "        print(f\"Preprocessed text of {filename}:\\n{preprocessed_text}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4hwd5n8KISr",
        "outputId": "c25cda30-3954-4502-fb49-dab73815930e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing complete for all files.\n",
            "Preprocessed text of file194.txt:\n",
            "superb feel sound tips wah success rock pedal way forward match volume knob bass crank q love life seriously range pedal hits q amazing love expressive tone get going think retains low end adds creamy wah highs mids listen claim lose lows put bluntly needed board used delays octave overdrives clean never found sound like could use buy\n",
            "\n",
            "Preprocessed text of file854.txt:\n",
            "great les paul like guitar novice player minimum setup needed string buzzed little everyone loves look fun play affordable price\n",
            "\n",
            "Preprocessed text of file329.txt:\n",
            "design tool great overall like flexibility job well going hear less sibilance vocals recorded definitely recommend every artist mixing engineer get clean quality recording cheap price big problem see pop filter studios days worry expensive advanced time get job done equipment keep project wo let\n",
            "\n",
            "Preprocessed text of file399.txt:\n",
            "guitar amazing value usa pups chunky neck fast fretboard round radius kind guitar never want put switching options make versatile criticism poly blonde finish plastic looking thankfully grain showing every day leave mine sun may repaint future guitar youll want keep life satisfied\n",
            "\n",
            "Preprocessed text of file430.txt:\n",
            "got hate changing strings floating tremolo absolutely first thought may made mistake tone super bright sounding first installation played half hour retuned sounded better played another half hour retuned sounded even better played another hours sound fantastic update get hundred hours days say handle bends well dive bombing deep trem hurt yet impressed\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUESTION 2 Unigram Inverted Index"
      ],
      "metadata": {
        "id": "89kpI4yeMdtg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import string\n",
        "import pickle\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def preprocess_document(text):\n",
        "    stop_words_set = set(stopwords.words('english'))\n",
        "    lowercase_text = text.lower()\n",
        "    no_punctuation_text = lowercase_text.translate(str.maketrans('', '', string.punctuation))\n",
        "    tokenized_words = word_tokenize(no_punctuation_text)\n",
        "    filtered_words = [word for word in tokenized_words if word not in stop_words_set]\n",
        "    return filtered_words\n",
        "\n",
        "def build_inverted_index(directory_path):\n",
        "    index = {}\n",
        "    for file_name in os.listdir(directory_path):\n",
        "        if file_name.endswith(\".txt\"):\n",
        "            full_path = os.path.join(directory_path, file_name)\n",
        "            with open(full_path, 'r', encoding='utf-8') as document:\n",
        "                document_content = document.read()\n",
        "                document_words = preprocess_document(document_content)\n",
        "                for word in document_words:\n",
        "                    if word not in index:\n",
        "                        index[word] = set()\n",
        "                    index[word].add(file_name)\n",
        "    return index\n",
        "\n",
        "def perform_query_operation(doc_set_1, doc_set_2, op):\n",
        "    if op == 'AND':\n",
        "        return doc_set_1.intersection(doc_set_2)\n",
        "    elif op == 'OR':\n",
        "        return doc_set_1.union(doc_set_2)\n",
        "    elif op == 'AND NOT':\n",
        "        return doc_set_1 - doc_set_2\n",
        "    elif op == 'OR NOT':\n",
        "        return doc_set_1 - doc_set_2\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown operation: {op}\")\n",
        "\n",
        "def persist_inverted_index(index, path_to_save):\n",
        "    with open(path_to_save, 'wb') as index_file:\n",
        "        pickle.dump(index, index_file)\n",
        "\n",
        "def retrieve_inverted_index(path_to_load):\n",
        "    with open(path_to_load, 'rb') as index_file:\n",
        "        return pickle.load(index_file)\n",
        "\n",
        "def handle_query(query_tokens, query_ops, index):\n",
        "    query_result = set()\n",
        "    if query_tokens:\n",
        "        initial_docs = index.get(query_tokens[0], set())\n",
        "        for operation_index, operation in enumerate(query_ops):\n",
        "            if operation_index < len(query_tokens) - 1:\n",
        "                subsequent_docs = index.get(query_tokens[operation_index + 1], set())\n",
        "                initial_docs = perform_query_operation(initial_docs, subsequent_docs, operation.strip())\n",
        "        query_result = initial_docs\n",
        "    return query_result\n",
        "\n",
        "def query_system():\n",
        "    index_path = '/content/drive/My Drive/IR ASSIGNMENT/inverted_index.pkl'\n",
        "    if os.path.exists(index_path):\n",
        "        doc_index = retrieve_inverted_index(index_path)\n",
        "    else:\n",
        "        docs_directory = '/content/drive/My Drive/IR ASSIGNMENT/preprocessed_text_files'\n",
        "        doc_index = build_inverted_index(docs_directory)\n",
        "        persist_inverted_index(doc_index, index_path)\n",
        "\n",
        "    query_count = int(input(\"Enter the number of Queries: \"))\n",
        "    for i in range(1, query_count + 1):\n",
        "        user_query = input(f\"Enter query {i}: \")\n",
        "        query_operations = input(\"Enter operations separated by comma for Query: \").split(',')\n",
        "        query_words = preprocess_document(user_query)\n",
        "        result_docs = handle_query(query_words, query_operations, doc_index)\n",
        "\n",
        "        # Formatting and printing the query and results\n",
        "        formatted_query = ' '.join([f\"{query_words[j]} {op}\" for j, op in enumerate(query_operations)] + [query_words[-1]])\n",
        "        print(f\"Query {i}: {formatted_query}\")\n",
        "        print(f\"Number of documents retrieved for Query {i}: {len(result_docs)}\")\n",
        "        if len(result_docs) > 0:\n",
        "            print(f\"Names of the documents retrieved for Query {i}: {', '.join(sorted(result_docs))}\")\n",
        "        else:\n",
        "            print(\"No documents retrieved.\")\n",
        "\n",
        "query_system()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v941d17mMizO",
        "outputId": "e883ef40-b50e-41dc-893f-b7ee2b05efdc"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the number of Queries: 2\n",
            "Enter query 1: Car bag in a canister\n",
            "Enter operations separated by comma for Query: OR, AND NOT\n",
            "Query 1: car OR bag  AND NOT canister\n",
            "Number of documents retrieved for Query 1: 31\n",
            "Names of the documents retrieved for Query 1: file118.txt, file166.txt, file174.txt, file264.txt, file3.txt, file313.txt, file363.txt, file404.txt, file459.txt, file466.txt, file542.txt, file573.txt, file665.txt, file682.txt, file686.txt, file698.txt, file699.txt, file73.txt, file738.txt, file746.txt, file780.txt, file797.txt, file860.txt, file863.txt, file864.txt, file886.txt, file892.txt, file930.txt, file942.txt, file956.txt, file981.txt\n",
            "Enter query 2: Coffee brewing techniques in cookbook\n",
            "Enter operations separated by comma for Query: AND, OR NOT, OR\n",
            "Query 2: coffee AND brewing  OR NOT techniques  OR cookbook\n",
            "Number of documents retrieved for Query 2: 0\n",
            "No documents retrieved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3"
      ],
      "metadata": {
        "id": "j8eou0kfB2kB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def preprocess_document(document_text):\n",
        "    stopwords_list = set(stopwords.words('english'))\n",
        "    lowercase_text = document_text.lower()\n",
        "    no_punctuation_text = lowercase_text.translate(str.maketrans('', '', string.punctuation))\n",
        "    word_tokens = word_tokenize(no_punctuation_text)\n",
        "    filtered_tokens = [word for word in word_tokens if word not in stopwords_list]\n",
        "    return filtered_tokens\n",
        "\n",
        "def persist_index(index, storage_path):\n",
        "    with open(storage_path, 'wb') as index_file:\n",
        "        pickle.dump(index, index_file)\n",
        "\n",
        "def retrieve_index(index_path):\n",
        "    with open(index_path, 'rb') as index_file:\n",
        "        return pickle.load(index_file)\n",
        "\n",
        "def handle_phrase_query(phrase, index):\n",
        "    query_tokens = preprocess_document(phrase)\n",
        "    if not query_tokens:\n",
        "        return 0, []\n",
        "\n",
        "    documents_with_phrase = set(index.get(query_tokens[0], {}).keys())\n",
        "    for token in query_tokens[1:]:\n",
        "        documents_with_phrase &= set(index.get(token, {}).keys())\n",
        "\n",
        "    matched_documents = []\n",
        "    for document in documents_with_phrase:\n",
        "        token_positions = [index[token][document] for token in query_tokens if document in index[token]]\n",
        "        for initial_position in token_positions[0]:\n",
        "            if all((initial_position + offset in token_positions[offset]) for offset in range(1, len(query_tokens))):\n",
        "                matched_documents.append(document)\n",
        "                break\n",
        "\n",
        "    return len(matched_documents), matched_documents\n",
        "\n",
        "def build_positional_index(preprocessed_dir):\n",
        "    positional_index = {}\n",
        "    for file_name in os.listdir(preprocessed_dir):\n",
        "        if file_name.endswith(\".txt\"):\n",
        "            document_path = os.path.join(preprocessed_dir, file_name)\n",
        "            with open(document_path, 'r', encoding='utf-8') as document_file:\n",
        "                document_content = document_file.read()\n",
        "                document_tokens = preprocess_document(document_content)\n",
        "                for position, token in enumerate(document_tokens):\n",
        "                    if token not in positional_index:\n",
        "                        positional_index[token] = {}\n",
        "                    if file_name not in positional_index[token]:\n",
        "                        positional_index[token][file_name] = []\n",
        "                    positional_index[token][file_name].append(position)\n",
        "    return positional_index\n",
        "\n",
        "\n",
        "def run_query_system():\n",
        "    index_file_path = '/content/drive/My Drive/IR ASSIGNMENT/positional_index.pkl'\n",
        "    documents_directory = '/content/drive/My Drive/IR ASSIGNMENT/preprocessed_text_files'\n",
        "\n",
        "    if os.path.exists(index_file_path):\n",
        "        document_index = retrieve_index(index_file_path)\n",
        "    else:\n",
        "        document_index = build_positional_index(documents_directory)\n",
        "        persist_index(document_index, index_file_path)\n",
        "\n",
        "    query_count = int(input(\"Enter the number of queries: \"))\n",
        "    for query_num in range(query_count):\n",
        "        search_phrase = input(f\"Enter phrase query {query_num+1}: \")\n",
        "        documents_count, documents_list = handle_phrase_query(search_phrase, document_index)\n",
        "\n",
        "        print(f\"Number of documents retrieved for query {query_num+1} using positional index: {documents_count}\")\n",
        "        if documents_count > 0:\n",
        "            print(f\"Names of documents retrieved for query {query_num+1} using positional index: {', '.join(documents_list)}\")\n",
        "        else:\n",
        "            print(\"No documents retrieved using positional index.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_query_system()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B70K8WcmB4a8",
        "outputId": "d9604360-04f7-4c7f-80c4-0cf2a005ac35"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the number of queries: 3\n",
            "Enter phrase query 1: it is a good in front for poutch\n",
            "Number of documents retrieved for query 1 using positional index: 0\n",
            "No documents retrieved using positional index.\n",
            "Enter phrase query 2: it is good in reliable for fit\n",
            "Number of documents retrieved for query 2 using positional index: 1\n",
            "Names of documents retrieved for query 2 using positional index: file9.txt\n",
            "Enter phrase query 3: it is a fit front poutch\n",
            "Number of documents retrieved for query 3 using positional index: 1\n",
            "Names of documents retrieved for query 3 using positional index: file9.txt\n"
          ]
        }
      ]
    }
  ]
}