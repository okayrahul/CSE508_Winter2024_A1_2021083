{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP8fzL2FG5Tyn6lwaELzaCH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/okayrahul/CSE508_Winter2024_A1_2021083/blob/main/IR_ASSIGNMENT_2021083.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mg6j7yZJ35L",
        "outputId": "5d123cb2-2963-4caa-ca77-9d58e391dec1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUESTION 1 DATA PREPROCESSING"
      ],
      "metadata": {
        "id": "IHPWkEk0MWA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import string\n",
        "import nltk\n",
        "import random\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Ensure NLTK resources are downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Path to the dataset folder on Google Drive\n",
        "dataset_path = '/content/drive/My Drive/IR ASSIGNMENT/text_files'\n",
        "preprocessed_path = '/content/drive/My Drive/IR ASSIGNMENT/preprocessed_text_files'\n",
        "\n",
        "# Create a directory for preprocessed files if it doesn't exist\n",
        "if not os.path.exists(preprocessed_path):\n",
        "    os.makedirs(preprocessed_path)\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
        "    # Remove punctuation\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# List all files\n",
        "all_files = os.listdir(dataset_path)\n",
        "\n",
        "# Randomly select 5 files for previewing their original and preprocessed content\n",
        "selected_files_for_preview = random.sample(all_files, 5)\n",
        "\n",
        "# Preprocess all files\n",
        "preprocessed_files = []\n",
        "\n",
        "for filename in all_files:\n",
        "    file_path = os.path.join(dataset_path, filename)\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        original_text = file.read()\n",
        "\n",
        "        preprocessed_text = preprocess_text(original_text)\n",
        "\n",
        "        # Save the preprocessed text\n",
        "        preprocessed_file_path = os.path.join(preprocessed_path, filename)\n",
        "        with open(preprocessed_file_path, 'w', encoding='utf-8') as preprocessed_file:\n",
        "            preprocessed_file.write(preprocessed_text)\n",
        "            preprocessed_files.append(preprocessed_file_path)\n",
        "\n",
        "    # Check if this file is among the randomly selected for preview\n",
        "    if filename in selected_files_for_preview:\n",
        "        print(f\"Original text of {filename}:\\n{original_text}\\n---\")\n",
        "        print(f\"Preprocessed text of {filename}:\\n{preprocessed_text}\\n===\\n\")\n",
        "\n",
        "print(\"Preprocessing complete for all files.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4hwd5n8KISr",
        "outputId": "b0eeaa1c-d5e6-4309-cf6d-3757cb5b228c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text of file902.txt:\n",
            "Beautiful, well-made strap. Matches my guitar nicely. Perfect.\n",
            "---\n",
            "Preprocessed text of file902.txt:\n",
            "beautiful strap matches guitar nicely perfect\n",
            "===\n",
            "\n",
            "Original text of file827.txt:\n",
            "Purchased the 5-hole pickguard in eggshell for my MIM Baja Telecaster. Honestly, I wasn't expecting much since I hated the cheap, white plastic version that came stock. Black Bakelite looked good, gold anodized aluminum looked better, but somehow this one just seems right. It's a heavy, vintage thickness, rigid, and very high quality. The eggshell color adds just enough warmth as well. There is no shielding so you may want to slap some copper foil on the back.\n",
            "\n",
            "Note that this is NOT for Squiers that are made in Japan. Should fit all USA and most MIM models.\n",
            "---\n",
            "Preprocessed text of file827.txt:\n",
            "purchased pickguard eggshell mim baja telecaster honestly expecting much since hated cheap white plastic version came stock black bakelite looked good gold anodized aluminum looked better somehow one seems right heavy vintage thickness rigid high quality eggshell color adds enough warmth well shielding may want slap copper foil back note squiers made japan fit usa mim models\n",
            "===\n",
            "\n",
            "Original text of file611.txt:\n",
            "I put a set of these on a Jag I finished up a few weeks ago and I'm thoroughly impressed.\n",
            "\n",
            "I used the mounting plates for install because even w/ a drill press and depth stop bits it's still a time consuming pain drilling those holes and the plates seem to provide secure and well aligned mounting w/ just the bushings clamping the tuners down.\n",
            "\n",
            "The action of the tuners is super smooth and accurate feeling w/ zero slippage and the button shape is very comfortable and effective.  They seem quicker to tune w/ for me than I would expect from the 18:1 ratio.  I think they look great too.\n",
            "\n",
            "I've liked them so much on the Jag I've got another set of these (and one of their Contour strat bridges) on the way for the next build.\n",
            "---\n",
            "Preprocessed text of file611.txt:\n",
            "put set jag finished weeks ago thoroughly impressed used mounting plates install even drill press depth stop bits still time consuming pain drilling holes plates seem provide secure well aligned mounting bushings clamping tuners action tuners super smooth accurate feeling zero slippage button shape comfortable effective seem quicker tune would expect ratio think look great liked much jag got another set one contour strat bridges way next build\n",
            "===\n",
            "\n",
            "Original text of file586.txt:\n",
            "I love this pickguard, looks so cool on my red Telecaster, matches all the other chrome hardware, made of metal and does NOT scratch! Every one that sees my Telecaster loves it! Thanks! Good price also, saw several other sellers with this product, this seller had the best price by far! Packaged VERY well and delivered in a timely manner!\n",
            "---\n",
            "Preprocessed text of file586.txt:\n",
            "love pickguard looks cool red telecaster matches chrome hardware made metal scratch every one sees telecaster loves thanks good price also saw several sellers product seller best price far packaged well delivered timely manner\n",
            "===\n",
            "\n",
            "Original text of file535.txt:\n",
            "Sticks on great. It also holds on extremely tight. And when i take it of my finish is perfect\n",
            "---\n",
            "Preprocessed text of file535.txt:\n",
            "sticks great also holds extremely tight take finish perfect\n",
            "===\n",
            "\n",
            "Preprocessing complete for all files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUESTION 2 Unigram Inverted Index"
      ],
      "metadata": {
        "id": "89kpI4yeMdtg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import string\n",
        "import pickle\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def preprocess_document(text):\n",
        "    stop_words_set = set(stopwords.words('english'))\n",
        "    lowercase_text = text.lower()\n",
        "    no_punctuation_text = lowercase_text.translate(str.maketrans('', '', string.punctuation))\n",
        "    tokenized_words = word_tokenize(no_punctuation_text)\n",
        "    filtered_words = [word for word in tokenized_words if word not in stop_words_set]\n",
        "    return filtered_words\n",
        "\n",
        "def build_inverted_index(directory_path):\n",
        "    index = {}\n",
        "    for file_name in os.listdir(directory_path):\n",
        "        if file_name.endswith(\".txt\"):\n",
        "            full_path = os.path.join(directory_path, file_name)\n",
        "            with open(full_path, 'r', encoding='utf-8') as document:\n",
        "                document_content = document.read()\n",
        "                document_words = preprocess_document(document_content)\n",
        "                for word in document_words:\n",
        "                    if word not in index:\n",
        "                        index[word] = set()\n",
        "                    index[word].add(file_name)\n",
        "    return index\n",
        "\n",
        "def perform_query_operation(doc_set_1, doc_set_2, op):\n",
        "    if op == 'AND':\n",
        "        return doc_set_1.intersection(doc_set_2)\n",
        "    elif op == 'OR':\n",
        "        return doc_set_1.union(doc_set_2)\n",
        "    elif op == 'AND NOT':\n",
        "        return doc_set_1 - doc_set_2\n",
        "    elif op == 'OR NOT':\n",
        "        return doc_set_1 - doc_set_2\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown operation: {op}\")\n",
        "\n",
        "def persist_inverted_index(index, path_to_save):\n",
        "    with open(path_to_save, 'wb') as index_file:\n",
        "        pickle.dump(index, index_file)\n",
        "\n",
        "def retrieve_inverted_index(path_to_load):\n",
        "    with open(path_to_load, 'rb') as index_file:\n",
        "        return pickle.load(index_file)\n",
        "\n",
        "def handle_query(query_tokens, query_ops, index):\n",
        "    query_result = set()\n",
        "    if query_tokens:\n",
        "        initial_docs = index.get(query_tokens[0], set())\n",
        "        for operation_index, operation in enumerate(query_ops):\n",
        "            if operation_index < len(query_tokens) - 1:\n",
        "                subsequent_docs = index.get(query_tokens[operation_index + 1], set())\n",
        "                initial_docs = perform_query_operation(initial_docs, subsequent_docs, operation.strip())\n",
        "        query_result = initial_docs\n",
        "    return query_result\n",
        "\n",
        "def query_system():\n",
        "    index_path = '/content/drive/My Drive/IR ASSIGNMENT/inverted_index.pkl'\n",
        "    if os.path.exists(index_path):\n",
        "        doc_index = retrieve_inverted_index(index_path)\n",
        "    else:\n",
        "        docs_directory = '/content/drive/My Drive/IR ASSIGNMENT/preprocessed_text_files'\n",
        "        doc_index = build_inverted_index(docs_directory)\n",
        "        persist_inverted_index(doc_index, index_path)\n",
        "\n",
        "    query_count = int(input(\"Enter the number of Queries: \"))\n",
        "    for i in range(1, query_count + 1):\n",
        "        user_query = input(f\"Enter query {i}: \")\n",
        "        query_operations = input(\"Enter operations separated by comma for Query: \").split(',')\n",
        "        query_words = preprocess_document(user_query)\n",
        "        result_docs = handle_query(query_words, query_operations, doc_index)\n",
        "\n",
        "        # Formatting and printing the query and results\n",
        "        formatted_query = ' '.join([f\"{query_words[j]} {op}\" for j, op in enumerate(query_operations)] + [query_words[-1]])\n",
        "        print(f\"Query {i}: {formatted_query}\")\n",
        "        print(f\"Number of documents retrieved for Query {i}: {len(result_docs)}\")\n",
        "        if len(result_docs) > 0:\n",
        "            print(f\"Names of the documents retrieved for Query {i}: {', '.join(sorted(result_docs))}\")\n",
        "        else:\n",
        "            print(\"No documents retrieved.\")\n",
        "\n",
        "query_system()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v941d17mMizO",
        "outputId": "98f3515a-6ee3-4186-a903-4ccbd26760f3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the number of Queries: 2\n",
            "Enter query 1: Car bag in a canister\n",
            "Enter operations separated by comma for Query: OR, AND NOT\n",
            "Query 1: car OR bag  AND NOT canister\n",
            "Number of documents retrieved for Query 1: 31\n",
            "Names of the documents retrieved for Query 1: file118.txt, file166.txt, file174.txt, file264.txt, file3.txt, file313.txt, file363.txt, file404.txt, file459.txt, file466.txt, file542.txt, file573.txt, file665.txt, file682.txt, file686.txt, file698.txt, file699.txt, file73.txt, file738.txt, file746.txt, file780.txt, file797.txt, file860.txt, file863.txt, file864.txt, file886.txt, file892.txt, file930.txt, file942.txt, file956.txt, file981.txt\n",
            "Enter query 2: Coffee brewing techniques in cookbook\n",
            "Enter operations separated by comma for Query: AND, OR NOT, OR\n",
            "Query 2: coffee AND brewing  OR NOT techniques  OR cookbook\n",
            "Number of documents retrieved for Query 2: 0\n",
            "No documents retrieved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3"
      ],
      "metadata": {
        "id": "j8eou0kfB2kB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def preprocess_document(document_text):\n",
        "    stopwords_list = set(stopwords.words('english'))\n",
        "    lowercase_text = document_text.lower()\n",
        "    no_punctuation_text = lowercase_text.translate(str.maketrans('', '', string.punctuation))\n",
        "    word_tokens = word_tokenize(no_punctuation_text)\n",
        "    filtered_tokens = [word for word in word_tokens if word not in stopwords_list]\n",
        "    return filtered_tokens\n",
        "\n",
        "def persist_index(index, storage_path):\n",
        "    with open(storage_path, 'wb') as index_file:\n",
        "        pickle.dump(index, index_file)\n",
        "\n",
        "def retrieve_index(index_path):\n",
        "    with open(index_path, 'rb') as index_file:\n",
        "        return pickle.load(index_file)\n",
        "\n",
        "def handle_phrase_query(phrase, index):\n",
        "    query_tokens = preprocess_document(phrase)\n",
        "    if not query_tokens:\n",
        "        return 0, []\n",
        "\n",
        "    documents_with_phrase = set(index.get(query_tokens[0], {}).keys())\n",
        "    for token in query_tokens[1:]:\n",
        "        documents_with_phrase &= set(index.get(token, {}).keys())\n",
        "\n",
        "    matched_documents = []\n",
        "    for document in documents_with_phrase:\n",
        "        token_positions = [index[token][document] for token in query_tokens if document in index[token]]\n",
        "        for initial_position in token_positions[0]:\n",
        "            if all((initial_position + offset in token_positions[offset]) for offset in range(1, len(query_tokens))):\n",
        "                matched_documents.append(document)\n",
        "                break\n",
        "\n",
        "    return len(matched_documents), matched_documents\n",
        "\n",
        "def build_positional_index(preprocessed_dir):\n",
        "    positional_index = {}\n",
        "    for file_name in os.listdir(preprocessed_dir):\n",
        "        if file_name.endswith(\".txt\"):\n",
        "            document_path = os.path.join(preprocessed_dir, file_name)\n",
        "            with open(document_path, 'r', encoding='utf-8') as document_file:\n",
        "                document_content = document_file.read()\n",
        "                document_tokens = preprocess_document(document_content)\n",
        "                for position, token in enumerate(document_tokens):\n",
        "                    if token not in positional_index:\n",
        "                        positional_index[token] = {}\n",
        "                    if file_name not in positional_index[token]:\n",
        "                        positional_index[token][file_name] = []\n",
        "                    positional_index[token][file_name].append(position)\n",
        "    return positional_index\n",
        "\n",
        "\n",
        "def run_query_system():\n",
        "    index_file_path = '/content/drive/My Drive/IR ASSIGNMENT/positional_index.pkl'\n",
        "    documents_directory = '/content/drive/My Drive/IR ASSIGNMENT/preprocessed_text_files'\n",
        "\n",
        "    if os.path.exists(index_file_path):\n",
        "        document_index = retrieve_index(index_file_path)\n",
        "    else:\n",
        "        document_index = build_positional_index(documents_directory)\n",
        "        persist_index(document_index, index_file_path)\n",
        "\n",
        "    query_count = int(input(\"Enter the number of queries: \"))\n",
        "    for query_num in range(query_count):\n",
        "        search_phrase = input(f\"Enter phrase query {query_num+1}: \")\n",
        "        documents_count, documents_list = handle_phrase_query(search_phrase, document_index)\n",
        "\n",
        "        print(f\"Number of documents retrieved for query {query_num+1} using positional index: {documents_count}\")\n",
        "        if documents_count > 0:\n",
        "            print(f\"Names of documents retrieved for query {query_num+1} using positional index: {', '.join(documents_list)}\")\n",
        "        else:\n",
        "            print(\"No documents retrieved using positional index.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_query_system()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B70K8WcmB4a8",
        "outputId": "22ab5f6b-1001-4eb8-fbf2-7c3c8356613d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the number of queries: 3\n",
            "Enter phrase query 1: it is a good in front for poutch\n",
            "Number of documents retrieved for query 1 using positional index: 0\n",
            "No documents retrieved using positional index.\n",
            "Enter phrase query 2: it is good in reliable for fit\n",
            "Number of documents retrieved for query 2 using positional index: 1\n",
            "Names of documents retrieved for query 2 using positional index: file9.txt\n",
            "Enter phrase query 3: it is a fit front poutch\n",
            "Number of documents retrieved for query 3 using positional index: 1\n",
            "Names of documents retrieved for query 3 using positional index: file9.txt\n"
          ]
        }
      ]
    }
  ]
}